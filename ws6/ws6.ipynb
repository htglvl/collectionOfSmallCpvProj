{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_3 = np.zeros([720,1080,3],dtype=np.uint8)\n",
    "img_3_align = np.zeros([720,1080,3],dtype=np.uint8)\n",
    "img_3.fill(255)\n",
    "img_3_align.fill(255)\n",
    "cv2.namedWindow(\"ws6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_image(images = []):\n",
    "    if len(images) == 1:\n",
    "        print(type(images[0]))\n",
    "        return images[0]\n",
    "    MIN_MATCH_COUNT = 4\n",
    "    print(f\"Number of image: {len(images)}\")\n",
    "    images_gray = []\n",
    "    images_keypoints = []\n",
    "    images_descriptions = []\n",
    "    bf = cv2.BFMatcher()\n",
    "    sift = cv2.SIFT_create()\n",
    "    for i in range(len(images)):\n",
    "        images_gray.append(cv2.cvtColor(images[i], cv2.COLOR_BGR2GRAY))\n",
    "        kp_temp, des_temp = sift.detectAndCompute(images[i],None)\n",
    "        images_keypoints.append(kp_temp)\n",
    "        images_descriptions.append(des_temp)\n",
    "    matchesES =  [0]\n",
    "    len_of_good_for_each_image = [0]\n",
    "    lst_of_good_kp_of_each_matches = [0]\n",
    "    for i in range(1, len(images)):\n",
    "        matchesES.append(bf.knnMatch(images_descriptions[0],images_descriptions[i], k = 2))\n",
    "        good_for_each_matches = []\n",
    "        for m,n in matchesES[i]:\n",
    "            if m.distance < .8*n.distance:\n",
    "                good_for_each_matches.append(m)\n",
    "        len_of_good_for_each_image.append(len(good_for_each_matches))\n",
    "        lst_of_good_kp_of_each_matches.append(good_for_each_matches)\n",
    "    index_of_most_match_image_with_image_0 = len_of_good_for_each_image.index(max(len_of_good_for_each_image)) \n",
    "    good = lst_of_good_kp_of_each_matches[index_of_most_match_image_with_image_0]\n",
    "    kp_ref = images_keypoints[0]\n",
    "    kp_align = images_keypoints[index_of_most_match_image_with_image_0]\n",
    "    align_image = images[index_of_most_match_image_with_image_0]\n",
    "    reference_image = images[0]\n",
    "    reference_image_grey = images_gray[index_of_most_match_image_with_image_0]\n",
    "    # check left right\n",
    "    reference_image_grey_left = reference_image_grey[:, :reference_image_grey.shape[1]//2]\n",
    "    reference_image_grey_right = reference_image_grey[:, reference_image_grey.shape[1]//2:]\n",
    "    kp_left, des_left = sift.detectAndCompute(reference_image_grey_left,None)\n",
    "    kp_right, des_right = sift.detectAndCompute(reference_image_grey_right,None)\n",
    "    kp_align, des_align = sift.detectAndCompute(align_image,None)\n",
    "\n",
    "    matches_left = bf.knnMatch(des_left,des_align, k = 2)\n",
    "    matches_right = bf.knnMatch(des_right,des_align, k = 2)\n",
    "    left = []\n",
    "    for m,n in matches_left:\n",
    "        if m.distance < .75*n.distance:\n",
    "            left.append(m)\n",
    "    right = []\n",
    "    for m,n in matches_right:\n",
    "        if m.distance < .75*n.distance:\n",
    "            right.append(m)\n",
    "    if len(left) < len(right):\n",
    "        reference_image, align_image = align_image, reference_image\n",
    "\n",
    "\n",
    "    # find the two image that have the most common keypoint\n",
    "    if len(good)>MIN_MATCH_COUNT:\n",
    "        src_pts = np.float32([ kp_ref[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ kp_align[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "        M, mask = cv2.findHomography(dst_pts,src_pts, cv2.RANSAC)\n",
    "        matchesMask = mask.ravel().tolist()\n",
    "        h,w = reference_image_grey.shape\n",
    "        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "        #dst = cv2.perspectiveTransform(pts,M)\n",
    "        aligned_image = cv2.warpPerspective(align_image, M, (reference_image.shape[1]+ align_image.shape[1], align_image.shape[0]))\n",
    "        cv2.imshow(f'aligned_image', aligned_image)\n",
    "\n",
    "\n",
    "        mask = 255 * np.ones(reference_image.shape, reference_image.dtype)\n",
    "        center = (reference_image.shape[1]//2, reference_image.shape[0]//2)\n",
    "        result = cv2.seamlessClone(reference_image, aligned_image, mask, center, cv2.NORMAL_CLONE)\n",
    "        cv2.imshow('result', aligned_image)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # aligned_image[0:align_image.shape[0], 0:align_image.shape[1]] = align_image\n",
    "        # aligned_image[0:reference_image.shape[0], 0:reference_image.shape[1]] = reference_image\n",
    "        print(aligned_image.shape)\n",
    "    else:\n",
    "        print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT))\n",
    "    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                   singlePointColor = None,\n",
    "                   matchesMask = matchesMask, # draw only inliers\n",
    "                   flags = 2)\n",
    "    img_match = cv2.drawMatches(reference_image,kp_ref,align_image,kp_align,good,None,**draw_params)\n",
    "    cv2.imshow('img_match' + str(random.randint(1,120)), img_match)\n",
    "    images.pop(index_of_most_match_image_with_image_0)\n",
    "    images.pop(0)\n",
    "    images.insert(0, result)\n",
    "    return stitch_image(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of image: 2\n",
      "(640, 960, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "while(True):\n",
    "    cv2.imshow('ws6', img_3)\n",
    "    k = cv2.waitKey(0)\n",
    "    if k == ord('f'):\n",
    "        for i in range(2):\n",
    "            image = cv2.imread(str(i+1) + '.jpg')\n",
    "            images.append(image)\n",
    "        img_match_all= stitch_image(images)\n",
    "        print(type(img_match_all))\n",
    "        cv2.imshow('img_match_all', img_match_all)\n",
    "    if k == ord('l'): # load\n",
    "        Num_of_image = int(input('How many image do you want to stitch?'))\n",
    "        for i in range(Num_of_image):\n",
    "            image_path = input(f'image {i} path: ')\n",
    "            image = cv2.imread(image_path)\n",
    "            cv2.imshow(f'image {i}', image)\n",
    "            images.append(image)\n",
    "    if k == 13: # enter key\n",
    "        img_match_all= stitch_image(images)\n",
    "        cv2.imshow('img_match_all', img_match_all)\n",
    "    if (k == ord('q')): # quit\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
